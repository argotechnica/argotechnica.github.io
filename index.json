[{"authors":null,"categories":null,"content":"I‚Äôm a researcher, technologist, and writer focused on the social and economic aspects of artificial intelligence and the Internet. I work for a securities regulator studying tech trends in the capital markets, such as blockchains and DAOs. Of course, the thoughts expressed on this personal website are my own and do not reflect those of my employers, past, present, or future. ‚ü∂ More about me and what you\u0026rsquo;ll find on this site.\n","date":1621209600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1621296000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I‚Äôm a researcher, technologist, and writer focused on the social and economic aspects of artificial intelligence and the Internet. I work for a securities regulator studying tech trends in the capital markets, such as blockchains and DAOs.","tags":null,"title":"Cory Salveson","type":"authors"},{"authors":["Cory Salveson"],"categories":["Personal"],"content":"üëã Hello, Internet person! I hope we won\u0026rsquo;t stay strangers. Like it says on the homepage, I‚Äôm a researcher, technologist, and writer focused on the social and economic aspects of artificial intelligence and the Internet. For example, I\u0026rsquo;ve researched how AI is changing lifelong learning, created video art online using machine learning, and written management advice for AI adoption. Currently, I work for a securities regulator studying tech trends in the capital markets, such as blockchains and DAOs.\nüß† Most of all, I\u0026rsquo;m deeply curious about the impact of information and communication technologies on human learning and knowledge. About AI, modern approaches to which are based on machine learning, we still don\u0026rsquo;t know everything AI models can learn. The same is true for us. With techniques for \u0026ldquo;graphing\u0026rdquo; knowledge and \u0026ldquo;processing\u0026rdquo; language, what can we now know? This question reaches beyond computer science and economics into philosophy and art. AI changes our relationship to ourselves.\nüëî Dramatic changes are also occuring in our economic relationships to each other and the earth. I like to say that digital transformation is social transformation. Markets‚Äîone of the oldest information technologies‚Äîare being updated with AI, blockchains, and digital platforms. These new technologies arguably exacerbate the great challenges of our age, such as inequality, climate change, and misinformation, yet they also enable new solutions. I\u0026rsquo;m proud to be working in this space at a regulator and public servant: fintech both requires and enables new regulatory approaches so that society benefits.\nüéì As a lifelong learner and bookworm, it was a dream come true to study at Oxford. As ever, my time there (2016-17) was multithreaded. Core coursework focued on the economics of new digital platforms, especially disruption and institutional change in the field of education. I also fell in love with knowledge graphs; honed skills in statistics, scientometrics, digital ethnography, and agent-based modeling; assisted Bodleian Libraries with publishing scholarly data to Wikidata using SPARQL; won a funding competition for innovative educational technology; and worked as a researcher on a Google-funded study about the future of AI and lifelong learning. Then and now, dominus illuminatio mea.\n‚ú® I hope to explore all of the above on this website, along with topics such as: video art, poetry, digital collecting and organizing, Python and data science, Linux and open source, role-playing games and storygames, speculative fiction, monasticism and contemplation, œÜŒπŒªŒøœÉŒøœÜŒØŒ±, the Commoner Earth, ‚ÅÇ‚Ä¶\n","date":1621209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621296000,"objectID":"214af6fa0e55e545cd347688ccaacc4d","permalink":"corysalveson.com/post/dear-reader/","publishdate":"2021-05-17T00:00:00Z","relpermalink":"corysalveson.com/post/dear-reader/","section":"post","summary":"A longer bio introducing my work and research interests. 1. We still don't know everything AI models‚Äîor we‚Äîcan learn. 2. Digital transformation is social transformation. 3. *Dominus illuminatio mea.*","tags":["personal","bio"],"title":"Dear reader","type":"post"},{"authors":["Huw C Davies","Rebecca Eynon","Cory Salveson"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607040000,"objectID":"a6f02ddd0cf1c046a164d277b62f3be4","permalink":"corysalveson.com/publication/davies-eynon-salveson-2020-mobilisation-of-ai-in-education/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"corysalveson.com/publication/davies-eynon-salveson-2020-mobilisation-of-ai-in-education/","section":"publication","summary":"We use knowledge graphs, an emerging technique in the field of AI, to critically examine how and why different stakeholders in education, educational technology and policy are valorising AI, the main concepts they collectively endorse, and their incentives for doing so.","tags":["knowledge graphs","AI","Bourdieu","sociology","education","OII"],"title":"The Mobilisation of AI in Education: A Bourdieusean Field Analysis","type":"publication"},{"authors":["Karthik Ramakrishnan","Cory Salveson"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1588809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588809600,"objectID":"24e55bab0c806514c4297438f123b36c","permalink":"corysalveson.com/publication/eai-ramakrishnan-salveson-2020-ai-maturity-framework/","publishdate":"2020-05-07T00:00:00Z","relpermalink":"corysalveson.com/publication/eai-ramakrishnan-salveson-2020-ai-maturity-framework/","section":"publication","summary":"A strategic guide to operationalize and scale enterprise AI solutions, featuring industry research into the state of AI maturity in multiple industries in 2019/2020. White paper published by Element AI in 2020.","tags":["knowledge graphs","AI","Bourdieu","sociology","education","OII"],"title":"The AI Maturity Framework","type":"publication"},{"authors":["Cory Salveson"],"categories":["Research"],"content":"This content was originally posted on the Oxford Internet Institute blog. I\u0026rsquo;ve lightly edited it here to streamline for readability and reflect changing terminology in the field.\nTable of Contents  Beginning with ends in mind Roadmapping the data analysis process Phase 1: Bibliometric data analysis of academic publications  Phase 1 data collection and storage tools Phase 1 data visualization and analysis tools Phase 1 tool discussion   Phase 2: Natural language analysis of academic publications  Phase 2 data collection and storage tools Phase 2 data visualization and analysis tools Phase 2 tool discussion   Network and natural language analysis of social media  Phase 3 data collection and storage tools Phase 3 data visualization and analysis tools Phase 3 tool discussion   Reflecting on analytical engineering    Beginning with ends in mind Following our introductory post about AI and lifelong learning, we wanted to focus on the technology stack we‚Äôre using to conduct our research. We thought this topic made sense now for two reasons:\n  To provide a single point of reference for anyone who wishes to engage with the software and code described in future publications; and\n  to be transparent about‚Äîand reflect upon‚Äîthe role of software in shaping the research process itself.\n  For example, we chose early on to incorporate machine learning techniques into our research: to address a research challenge we encountered, but also to gain firsthand experience using AI for augmenting human intelligence. If we‚Äôre going to assess claims about how AI can be used for learning, after all, it seems sensible for us to gain experience applying AI in this space ourselves!\nRoadmapping the data analysis process Again, we‚Äôre interested in mapping not only the breadth of discourse about AI and lifelong learning, but especially the underexplored relationships between these subjects. This makes for a lot of material to potentially review, much of it from related but distinct communities.\nAs a result, the object of our analysis came into focus quickly: documents such as journal articles, press releases, social media, and unstructured web content. Tools and methods for analyzing documents took more time and experimentation to develop. As of today, we identify three phases of analytical foci and corresponding tools:\n  ‚ÄúOut of the box‚Äù tools for scientometric analysis of structured document metadata;\n  A common Python natural language processing (NLP) pipeline for analyzing semi-structured document data; and\n  A bespoke graph database platform (knowledge graph) for network analysis of unstructured document data.\n  We‚Äôll share the output of these analyses per se in future posts. For the rest of this post, we focus on what tools we chose in each phase, how, and with what lessons learned.\nTo help organize each phase, I‚Äôve found it helpful to distinguish between two sets of data management tasks:\n  Data collection and storage: How are data gathered? Once downloaded or captured, how is it stored over time to support visualization and analysis? For example, should it be loaded into a database system, or stored directly in memory as a Python object? How will I share it, protect it, back it up?\n  Data visualization and analysis: Anscombe‚Äôs quartet teaches us that data visualization belongs square in the middle of analysis efforts‚Äînot just as a tool for communicating findings at the end. So, what capabilities are available for visualizing and analyzing data throughout the process? How can these capabilities be adapted, modified, recombined, etc.?\n  For each phase, I first present tools by sub-task, then discuss how all tools fit together within the phase. Every tool we used was free and/or open-source software, and (with a little patience) can all run on Windows, Mac, and Linux. I encourage you to check them out if you haven‚Äôt already!\nPhase 1: Bibliometric data analysis of academic publications We started with academic publications, which benefit from readily available data as well as interpretive standards established through scientometrics, the quantitative study of research.\nPhase 1 data collection and storage tools  Harzing‚Äôs Publish or Perish: Bulk data collection tool created by Anne-Wil Harzing, now in its 6th release, with support for Google Scholar, Microsoft Academic, and Crossref. Also helpful for calculating standard bibliometric scores, saving and comparing searches. JabRef: Bibliographic data management tool focused on managing entries as a BibTeX file. Features for deduplicating entries.  Phase 1 data visualization and analysis tools  VOSviewer: Visualization tool for bibliometric networks as well as keyword/term network visualization. Makes it easy to start finding topical patterns based on abstracts. Some features dependent on file format of data input files.  Phase 1 tool discussion We first targeted bulk data collection of bibliometric metadata, such as article and journal titles, authors, keywords, etc., using Harzing‚Äôs Publish or Perish, a bulk bibliographic metadata collection tool. The bibliographic data manager JabRef allowed us to merge data files from different sources into a single dataset using the BibTeX standard. We also used JabRef to de-duplicate entries as much as possible‚Äîa significant challenge given sometimes thousands of overlapping entries from multiple databases. Finally, VOSviewer allowed us to visualize patterns of usage of terms found in article abstracts.\nThough quick and easy to get these ‚Äúout of the box‚Äù tools working, the approach presented three main drawbacks:\n  Duplicate records. JabRef‚Äôs deduplication feature didn‚Äôt quite scale to tens of thousands of documents.\n  Support for citation network analysis. The representation of citation data was too inconsistent and sparse across the dataset to allow us to use VOSviewer the way we hoped.\n  Support for subsetting data. We wanted to be able to visualize subsets of data, but the process was labor-intensive with JabRef. We also wished for better presrevation of provenance within the dataset, such as database and search term of origin.\n  In short, while Phase 1 proved out our basic approach for triangulating discourse, the actual process of downloading from multiple data sources, compiling into a monolithic file in JabRef, then exporting to VOSviewer, was too error prone and labor intensive to be sustainable, given our goals. For example, if there are distinct topic ‚Äúnetworks‚Äù emerging, what disciplines are the source articles/journals in? Do journal disciplines correspond to topic networks? Etc.\nFor Phase 2, we attempted to streamline these steps from a process designed around a monolithic dataset and analysis step, into more of an iterative search process involving permutations of the dataset itself and of analyical techniques applied to it.\nPhase 2: Natural language analysis of academic publications We turned to topic modeling and text classification techniques, paired with specialized visualizations and a bespoke ‚Äúreport generation‚Äù approach, to develop a more qualitative and expressive analysis of the topic space.\nWhile we did reuse the BibTeX data standard from Phase 1, we shifted to using the Python 3 Anaconda Distribution to take advantage of multiple community packages dedicated to various sub-tasks in the phase. The tools below correspond to these packages.\nPhase 2 data collection and storage tools  Python ‚Äì bibtexparser: Adds helper functions for importing and exporting BibTeX files. Python ‚Äì Pandas: De facto standard for adding capabilities for working with tabular data using a ‚Äúdataframe‚Äù concept. Includes helper functions for import, export, and manipulation, including filtering/subsetting of data. Python ‚Äì NLTK: Natural Language Toolkit (NLTK) implements tasks in natural language processing at a fine level of granularity and control. Python ‚Äì scikit-learn: Most popular ‚ÄúSciPy Toolkit‚Äù collecting production-class implementations of machine learning algorithms, wrapped in an elegant ‚Äúpipeline‚Äù framework that allows for easy experimentation and configuration of ML workflows.  Phase 2 data visualization and analysis tools  Python ‚Äì seaborn: Data visualization library designed to streamline and extend the features of matplotlib, a common data visualization library for Python inspired by MATLAB. Python ‚Äì python-docx: Toolkit for creating and editing Office Open XML Document documents, AKA Microsoft Word documents. Python ‚Äì pyLDAvis: Generates interactive visualizations of latent Dirichlet allocation (LDA) topic models using HTML. Implements an R package, LDAvis.  Phase 2 tool discussion In our shift from article metrics to article abstracts, we wanted to ensure we could rapidly explore the parameter space of multiple dimensions in combination with each other:\n  Article provenance, such as search term used to obtain the article\n  NLP tasks (such as topic modeling) and approaches (such as distinct algorithms for performing topic modeling)\n  Parameters of specific approaches (such as number of topics the topic modeling approach should create)\n  To achieve this, we designed a pipeline of processing steps in Python that allowed us to tweak parameters at multiple points in the pipeline and quickly assess impact using interactive and static outputs. We did this by first bringing in bibliographic data using bibtexparser, then converting it to a Pandas dataframe for further processing. For example, Pandas allowed us to apply regular expression matching to filter and subset data.\nAfter this filtering step, a ‚Äúcreate topic report‚Äù function applies a series of transformations to the data before outputting an interactive topic model visualization using pyLDAvis, as well as a detailed Word document report (created with python-docx) for each topic containing the following:\n List of topics with descriptive statistics such as number of articles per topic, and distribution of topics per year, visualized using seaborn For each topic, listing of the top n articles within that topic group, including title, authors, journal, and abstract  To arrive at these report outputs, a number of NLP tasks are chained together. NTLK removes stopwords and lemmatizes article abstracts. Next, data is copied across two parallel processing flows using scikit-learn. For each flow, text data is converted into into a matrix representation suitable for quantitative and statistical analysis by topic modeling algorithms. These included term frequency-inverse document frequency (tf‚Äìidf) to support latent Dirichlet allocation (LDA), as well as term frequency to support Non-Negative Matrix Factorization (NMF).\nFor both LDA and NMF data flows, each document receives a score describing how well it fits within each of a given number of topic groups. Rather than settle on a single, ‚Äúcorrect‚Äù number of topics, we wanted to explore the effect of varying topic number to see what patternsemerged. Therefore, for each time the pipeline is run with a given data input, it iterates across multiple values for topic number, creating a distinct report file for each group count, e.g. reports for 3, 5, 7, 10, 20, and 40 topics. By reviewing reports across the parameter space of topic numbers, we could then isolate topic groups that were especially unique; persistent across topic numbers; or else irrelevant to our cause. This last category enabled us to prune irrelevant articles from our dataset, then re-iterate to analyze again.\nThis qualitative, exploratory approach to data analysis would not have been possible with out of the box tools. At the same time, while helpful in enabling us to grasp the breadth of topical foci within the space, it was less clear how to understand the social situatedness of topics, articles, or journals. For this, we turned to a more ambitious software pipeline in our third phase.III.\nNetwork and natural language analysis of social media In Phase 3, we wanted to take advantage of recent advances on a NLP task known as ‚Äúentity recognition‚Äù to build out a network analysis of activities occurring in social media related to AI and lifelong learning. That is, by collecting news articles; blogs/microblogs; and possibly the academic articles we had collected in Phases 1-2, we wanted to develop a semi-automated way of identifying what was being discussed in the various articles, as well as what social actors we could therefore deduce or infer were collaborating in some way. We envisioned a graph database system to serve as a knowledge base for tracking these insights, with a data collection and analysis system on top of this database to help populate it.\nThis phase is a work in progress, so this list is subject to change!\nPhase 3 data collection and storage tools  Neo4j (Community Edition): Graph database management system with a mature ecosystem of development tools, such as a Python driver (Py2neo); dedicated query language (Cypher; and helper tools (Awesome Procedures On Cypher AKA APOC). Readily available documentation. Graphileon Interactor (Community Edition): Visual interface for Neo4j that provides ad-hoc querying and data editing within a visual web interface. Python ‚Äì feedparser: Extracts structured data from RSS, ATOM, and other syndication feeds. Python ‚Äì Newspaper3k: Extracts structured data from websites containing serialized data in a ‚Äúnews article‚Äù format. Complements feedparser.  Phase 3 data visualization and analysis tools  Python ‚Äì spaCy: Newer NLP framework with more streamlined as well as advanced functionality than previous combination of NLTK + scikit-learn. Ships with trained models that achieve state of the art performance in multiple NLP tasks. Good documentation. Cytoscape: Graph/Network visualization software. Interfaces with Neo4j.  Phase 3 tool discussion For Phase 3, the vision is to use Python tools like feedparser and Newspaper3k to collect data from the open web. This will be stored within a Neo4j database in such a way as to preserve provenance (data source) represented as graph connections between data sources and documents. Using spaCy, we can then use named entity recognition (NER) functionality to identify nouns such as companies, products, and locations, from article texts. These can also be represented as distinct ‚Äúentity‚Äù graph nodes connected to document nodes. By analyzing within- as well as across-document mentions of specific products, companies, etc., we can identify a ‚Äúcollaboration network‚Äù within the space. Challenges related to pruning, enhancing, or creating data entries in the database are met by providing Graphileon Interactor to the general research team, since this tool provides ad-hoc querying and data creation/delete/editing capabilities. Finally, Cytoscape provides more advanced capabilities around visualizing and analyzing the graph itself.\nWe are actively working on this phase and hope to dedicate a future blog post to its progress.\nReflecting on analytical engineering Over the course of these three phases of research, it has struck me that although our choice of tools has always been lead by research questions, so too have our questions been lead by technical capabilities. This give and take of course also characterizes the application of AI for lifelong learning: certain tasks are becoming increasingly efficient and effective for computers to perform‚Äîbut what are the ‚Äúright‚Äù applications of AI techniques, on balance? (‚ÄúWhere is the knowledge we have lost in information?‚Äù)\nFor us, an important part of navigating this question has been to ensure we maintain a ‚Äúhuman in the middle‚Äù approach to our use of machine learning and other computational techniques. By this, we mean more than just the ‚Äúart‚Äù or pragmatic dimension of applying unsupervised learning techniques like topic clustering. Rather, we mean that qualitative checks like our Phase 2 topic reports were important to ensure, regardless of the technical performance of processing steps per se, that we had opportunities to leverage (and develop) our own intuition, creativity, and expertise in the space to make further decisions and conclusions. Though I risk anthropomorphizing AI by saying so, I‚Äôm inclined to characterize this as a ‚Äúpartnering with‚Äù relation to AI technologies, rather than a ‚Äúhand off work to‚Äù relation.\nAs the project has evolved, we‚Äôve also answered the concerns raised in this post by pursuing two other branches of activity: a more conventional literature review and synthesis of policy related to AI and lifelong learning, and a plan for case studies applying a more ethnographic approach to studying the use of AI for lifelong learning in situ. We hope you‚Äôll join us as we present these and more over coming weeks and months.\n","date":1529884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621296000,"objectID":"0535f2b17ab55003c39800d44715d1ad","permalink":"corysalveson.com/post/notes-on-analytical-engineering/","publishdate":"2018-06-25T00:00:00Z","relpermalink":"corysalveson.com/post/notes-on-analytical-engineering/","section":"post","summary":"The tools and techniques used throughout a project on AI and lifelong learning, why we chose them, and how you can get started using the same.","tags":["research","OII","AI","lifelong learning","education","network science"],"title":"Notes on analytical engineering","type":"post"},{"authors":["Rebecca Eynon","Cory Salveson"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1526256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526256000,"objectID":"10efc7a55a23c875034dcccfd1b593fb","permalink":"corysalveson.com/publication/eynon-salveson-2018-mapping-ai-and-education-debates/","publishdate":"2018-05-14T00:00:00Z","relpermalink":"corysalveson.com/publication/eynon-salveson-2018-mapping-ai-and-education-debates/","section":"publication","summary":"The role of artificial intelligence for learning is again attracting attention in policy and academic fields; a renaissance fuelled in part by the proliferation and availability of big data, alongside advances in computational techniques and the need for a new ‚Äòtechnical fix‚Äô for Education (Robins  and Webster, 1989). In the public domain, dramatic headlines abound proclaiming the end of education as we know it in utopian and dystopian terms. Yet, in the academic sphere important advances are being made that educators need to pay attention to in order to have a more nuanced and ‚Äòresponsible response‚Äô (Biesta, 2013) to the role that artificial intelligence can and should play in Education. This presentation aims to contribute to that goal through reporting findings from an ongoing study that aims to identify and explore academic studies that are concerned with artificial intelligence and Education. Through the use of a number of machine learning techniques we aim to map and visualise the current areas of research in this area and identify the underlying philosophies of learning and education embedded within these activities, drawing on Anna Sfard‚Äôs acquisition and participation metaphors for learning (Sfard, 1998). Through primarily computational analysis (including network analysis and natural language processing) of the citations, titles and abstracts (where available) of around 8500 books, chapters, papers and conference presentations alongside small scale qualitative coding of a sample of papers we highlight the different ways that people define and talk about AI in Education and demonstrate how the vast majority of work in this area is primarily promoting an ‚Äòacquisition‚Äô based view of learning, promoting individual cognition over collaborative, networked forms of participation. We argue that while this is not necessarily a problem as acquisition is an important aspect of learning; discussions of the use of artificial intelligence in Education would be significantly advanced if far more attention was placed on ways of thinking about learning and Education that promote a broader social-cultural view. This would enable more discussion of if, and how, the use of artificial intelligence in Education could advance knowledge in a Network Society alongside the use of artificial intelligence to make knowledge transfer more efficient; and further advance theoretical debates in Networked Learning.","tags":["learning","AI","education","OII"],"title":"Mapping AI and Education debates: revisiting acquisition and participation metaphors for learning","type":"publication"},{"authors":["Cory Salveson"],"categories":["Notes \u0026 Commentary"],"content":"The Pew Research Center released a timely new report today titled The Future of Jobs and Jobs Training. Announced on their blog as \u0026ldquo;Experts on the Future of Work, Jobs Training and Skills,\u0026rdquo; the report features survey data and selected quotes from some of my personal heroes, including danah boyd, Cory Doctorow, and Richard Stallman. It also features a quote from me!\nWritten during my time as Learning Systems and Analytics Lead at RSM US, I wrote:\n The nature of work today, and in future, is such that if people want to keep increasingly scarce well-paying jobs, they will need to educate themselves in an ongoing manner for their whole lives.\n I stand by that comment, but thought I\u0026rsquo;d expand on it a bit here, as I did on Twitter.\nNamely, I\u0026rsquo;d like to add that although savvy workers can, and I think all workers eventually must, own their own lifelong learning; I firmly believe that the institutionalization of lifelong learning needs to be debated at a broader societal level, i.e. in terms of policy and funding.\nWhat do I mean by \u0026ldquo;institutionalization\u0026rdquo;? I like the definition set out by M√©nard \u0026amp; Shirley in their Handbook of New Institutional Economics (2008; emphasis mine):\n Institutions are the written and unwritten rules, norms and constraints that humans devise to reduce uncertainty and control their environment. These include (i) written rules and agreements that govern contractual relations and corporate governance, (ii) constitutions, laws and rules that govern politics, government, finance, and society more broadly, and (iii) unwritten codes of conduct, norms of behavior, and beliefs.\n For example, if lifelong learning shifts from optional to compulsory, who should pay for the time investment of individuals and educators? Some professions already require lifelong learning, such as healthcare and accounting; but many don\u0026rsquo;t. In either case, the proliferation of free or very cheap, easily accessible education online (this cheapness itself being a kind of unwritten rule of the MOOC marketplace) seems to be headed in the direction of pressuring workers to use their own time to pursue lifelong learning. Is that fair? Maybe it (almost) was in a world in which a degree obtained in your twenties carried you through an entire career, and jobs existed at multiple, lower levels of educuation that provided living wages; but when the nature of work is such that lifelong education and learning are compulsory, it seems more suspect.\nThe key, then, is that we must not endanger the dignity and rights of workers by failing to transition learning institutions to the new normal of work. As suggested above, my intuition is that simply making more low-cost training available, such as through Massive Open Online Courses (MOOCs), is not enough in this regard.\nOverall, these concerns are some of the many that inspire my Master\u0026rsquo;s thesis at the Oxford Internet Institute this year. In the thesis, I hope to explore new institutional arrangements within higher education, such as what new policies (like the above), formal and informal rules, and organizations or organizational forms are developing online. I hope to share more as I work over the coming months, but in the meantime, I have a slideshow here describing the thesis proposal, if you\u0026rsquo;re curious.\nAs always, please do let me know if you have any comments or questions‚Äîparticularly research suggestions! You can reach me via my contact info on this website (including my C.V., which is now posted here), or catch me on Twitter.\n","date":1491591600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491591600,"objectID":"73b15bb5e89b02242e823eaa27aef802","permalink":"corysalveson.com/post/future-of-work/","publishdate":"2017-04-07T19:00:00Z","relpermalink":"corysalveson.com/post/future-of-work/","section":"post","summary":"Some comments expanding on my quote featured in the Pew Research Center report, [The Future of Jobs and Jobs Training](http://www.pewinternet.org/2017/05/03/the-future-of-jobs-and-jobs-training/).","tags":["learning","education","policy"],"title":"On lifelong learning and the future of work","type":"post"},{"authors":["Cory Salveson"],"categories":["Technology"],"content":"Background I ran Linux as my primary desktop for years, but now I find myself, as in a forest dark, running Windows 10. Bash on Ubuntu on Windows (BUW) is a commendable project, but it\u0026rsquo;s incomplete: BUW has a fully stocked /bin directory, for example, but ICMP is broken, so ping and traceroute don\u0026rsquo;t work. Some Python scripts also fail in ways unique to the BUW Linux environment. So, while I do hope the project goes on to get future updates, I need something a bit more powerful and flexible in the meantime.\nRunning a Linux virtual machine (VM) is my solution. Being a \u0026ldquo;real\u0026rdquo; Linux server, and one that I can fully control, it should be able to do anything I need for local (non-cloud) tasks. Meanwhile, the performance and resource costs of running a complete but lightweight VM, compared to BUW, are acceptable given the benefits. Unfortunately, support for things like mouse scrollback and clipboard sharing don\u0026rsquo;t work in VirtualBox (the free virtualization platform I prefer) when the guest is just a shell. I\u0026rsquo;ve also fallen in love with cmder, a persistent drop-down console environment that supports arbitrary shells, that I use constantly. Having to switch to a special, dedicated window for running commands on the VM is nearly a deal-breaker.\nRunning a headless VM (i.e. no display, real or virtual, attached), to which I can SSH from a shell in cmder, is therefore my ideal setup, with one key problem left to sort out: how to secure command-line SSH access to the Linux VM without BUW (which this effort undertakes to eliminate). Cygwin, \u0026ldquo;a distribution of popular GNU and other Open Source tools running on Microsoft Windows,\u0026rdquo; solves this problem while also providing additional benefits. So, starting from a setup where I was previously running a Windows command prompt as well as BUW in Cmder; with the occasional manual bootup of a VirtualBox VM in a separate window; I now have just one shell in Cmder provided by Cygwin, and‚Äîwith help from a shell script I\u0026rsquo;m calling \u0026ldquo;Ubik\u0026rdquo;‚Äîan easy to use, \u0026ldquo;real\u0026rdquo; Linux server for when I need it, all in the same Cmder window.\nSetup Cmder is a big deal to me, but not required for the basic approach taken here, so I\u0026rsquo;ve listed it in the \u0026ldquo;Optional Steps\u0026rdquo; below.\nPrerequisites Setting this up requires a recent version of Windows (I\u0026rsquo;m on Windows 10) and a basic level of comfort with Linux and virtual machine management concepts. For example, I don\u0026rsquo;t detail how to install the software below; rather, I show how to configure everything to work nicely together. If there\u0026rsquo;s demand for a more step-by-step intro to this content, I\u0026rsquo;d be happy to put something together‚Äîjust let me know!\nRequired software (versions I used listed for reference)  VirtualBox - v5.1.18 Linux virtual machine on VirtualBox - Ubuntu Server 16.04.2 LTS Cygwin - 64-bit version  Optional software  Cmder - v161206 stable  Step 1: Configure port forwarding on the Linux VM This enables SSH access to the VM.\n Shut down and close the VM if it isn\u0026rsquo;t closed already. Open up the Settings panel for the VM, navigating to Settings \u0026gt; Network. In the \u0026ldquo;Attached to:\u0026rdquo; dropdown, select NAT if not already selected. Expand the Advanced section, then click on Port Forwarding to open the Port Forwarding Rules dialog. Add a new port forwarding rule. Give it a handy name (\u0026ldquo;ssh\u0026rdquo;), leave Protocol set to \u0026ldquo;TCP\u0026rdquo;, leave Host IP blank, assign a Host Port (I\u0026rsquo;m using 3022), leave Guest IP blank, and set Guest Port to 22. Alternatively, you can accomplish this via command-line like so (my VM\u0026rsquo;s name is \u0026ldquo;Ubuntu\u0026rdquo;):  VBoxManage modifyvm Ubuntu --natpf1 \u0026quot;ssh,tcp,,3022,,22\u0026quot;\r Step 2: Streamline launch and login of the Linux VM VirtualBox understands the concept of running a \u0026ldquo;headless\u0026rdquo; server, i.e. running the machine without a (virtual) monitor attached, and this can be controlled via command-line as follows (my machine name is \u0026ldquo;Ubuntu\u0026rdquo;):\nVBoxManage startvm Ubuntu --type headless\r You can also stop a VM with the \u0026ldquo;save state\u0026rdquo; option via command-line; this is a feature for similar to putting your laptop to sleep by closing the lid, in which the machine state is saved in a snapshot:\nVBoxManage controlvm Ubuntu savestate\r To streamline this process, I\u0026rsquo;ve created a shell script I call ubik after Philip K. Dick\u0026rsquo;s novel of the same name. In the novel, the main character comes to question whether his reality is simulated or real after a series of distortions and other events. A product called Ubik, distributed in an aerosol can, is a mysterious substance that he and other characters discover to help stabilize the boundaries between different realities. Riffing on this idea, the shell script first checks which \u0026ldquo;reality\u0026rdquo; it\u0026rsquo;s running from (whether it\u0026rsquo;s being run from the guest or host), then transitions the user to the other one, starting or stopping the VM as needed and tidying up after itself. You can download it from GitHub here: https://github.com/argotechnica/ubik.\nHere\u0026rsquo;s some example usage of ubik:\nCory@LYRETOME /cygdrive/c/CMD\r$ ubik\rUbik ... Safe when taken as directed.\rWaiting for VM \u0026quot;Ubuntu\u0026quot; to power on...\rVM \u0026quot;Ubuntu\u0026quot; has been successfully started.\rAttempting to SSH to VM \u0026quot;Ubuntu\u0026quot;...\rWelcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-71-generic x86_64)\r* Documentation: https://help.ubuntu.com\r* Management: https://landscape.canonical.com\r* Support: https://ubuntu.com/advantage\r0 packages can be updated.\r0 updates are security updates.\rLast login: Fri Apr 7 13:10:18 2017 from 10.0.2.2\rcory@ubuntu:~$ ./ubik\rConnection to localhost closed.\rReturn the VM to cryonic suspension? y\r0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%\rCory@LYRETOME /cygdrive/c/CMD\r$\r If you put a copy of this script into your Cygwin and VM home users' PATH directories; grant execution permission to the script with chmod +x ubik; and edit the script to incorporate your relevant parameters, you can just type ubik from the command line, and it\u0026rsquo;ll take care of the rest.\nOptional Steps Configure Cmder with Cygwin If a drop-down console sounds appealing to you, install Cmder and then check out this guide for instructions on configuring Cygwin with ConEmu, the upstream project from which Cmder is built: https://github.com/nadav-dav/Cygwin-guide. Note that the author says to use the path C:\\cygwin\\Cygwin.bat, but your path may actually be C:\\cygwin64\\Cygwin.bat if you installed the 64-bit version of Cygwin.\nConfigure key-based SSH authentication This saves you from having to type passwords all the time. Canonical maintains a good writeup here. Essentially, in Cygwin, you can just do this:\nmkdir ~/.ssh\rchmod 700 ~/.ssh\rssh-keygen -t rsa\r Press \u0026ldquo;Enter\u0026rdquo; for every option to get a key that doesn\u0026rsquo;t require a passphrase. Then, copy the content from one of the resulting files, id_rsa.pub, into another file, authorized_keys, located on the VM. You can do this in one command (changing your port, username, and hostname as needed) like this:\ncat ~/.ssh/id_rsa.pub | ssh -p 3022 cory@localhost \u0026quot;cat \u0026gt;\u0026gt; ~/.ssh/ authorized_keys\u0026quot;\r You should be prompted for the password of the destination user account on the virtual machine for this command, but if it worked correctly, you shouldn\u0026rsquo;t have to enter a password to SSH ever again.\nStart Cmder automatically with Windows Create a shortcut to the Cmder executable and put the shortcut in your Startup folder in the Start Menu. Anything placed in this folder starts automatically when your computer boots up. Windows has worked like this for years, but for some reason the folder is hidden in Windows 10. Here\u0026rsquo;s the full path, so you don\u0026rsquo;t have to hunt for it:\n%APPDATA%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\nDiscussion Over the years, I\u0026rsquo;ve found that investing a few hours in optimizing my tech setup pays for itself many times over in future efficiencies‚Äîand opens the door for more creativity. (When I know the cost of doing x is just a few seconds and an easy-to-use command, rather than an unknown number of minutes and a memory exercise, I\u0026rsquo;m more likely to experiment with x.) In that spirit, the method proposed here sought to balance the best of three approaches for bringing Linux into Windows: the lightweight, compiled-for-Windows tools of Cygwin, the power of a \u0026ldquo;real\u0026rdquo; (virtual) Linux server, and (optionally) a fancy console interface called Cmder‚Äîwith a bit of ubik to help it all gel.\nReferences  https://askubuntu.com/questions/307677/constantly-check-if-the-virtualbox-is-started-or-still-booting-up-from-a-script https://en.wikipedia.org/wiki/Ubik https://github.com/nadav-dav/Cygwin-guide https://schier.co/blog/2013/03/13/start-virtualbox-vm-in-headless-mode.html https://stackoverflow.com/questions/5906441/how-to-ssh-to-a-virtualbox-guest-externally-through-a-host  ","date":1491591600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491591600,"objectID":"4de0b91872007c94163f0343c5b817ae","permalink":"corysalveson.com/post/headless-with-ubik/","publishdate":"2017-04-07T19:00:00Z","relpermalink":"corysalveson.com/post/headless-with-ubik/","section":"post","summary":"How and why I switched from [Bash on Ubuntu on Windows](https://msdn.microsoft.com/en-gb/commandline/wsl/faq) to Cygwin and VirtualBox with help from Philip K. Dick. A technical and philosophical post about my approach to Linux tooling on Windows.","tags":["technical","tools","Linux","VirtualBox"],"title":"Running headless with Ubik","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"corysalveson.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"corysalveson.com/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]